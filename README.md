# AI for Engineers
This repository guides you to learn and build a solid foundation for mathematics behind Artificial Intelligent Systems, you would visualize fundamentals and their use cases.
The course is divided into Tiers and the tiers only dictate important of the topic to AI engineering, it is not a knowledge tier but the importance to your foundation of AI engineering.

## Tier 1 — Core Pillars (absolutely essential, used everywhere)
### Rank	Topic	Why It’s Critical
1. Eigenvalues and Eigenvectors	Appear in stability analysis, PCA, covariance, dynamic systems, and neural network Jacobians.	
2. Rayleigh Quotients & Generalized Eigenvalues	Used in optimization, stability margins, vibration modes, and spectral learning.	
3. Gradient Descent Toward the Minimum	Fundamental to all deep learning and control optimization.	
4. Stochastic Gradient Descent and ADAM	Every AI model and robot learning pipeline uses these.	
5. Probability Distributions	Foundation of uncertainty modeling, SLAM, sensor fusion, and Bayesian robotics.	
6. Covariance Matrices and Joint Probabilities	Defines uncertainty propagation and estimation (EKF, UKF, etc.).	
7. Markov Chains	Core for reinforcement learning, planning, and temporal models.	
8. Backpropagation and the Chain Rule	Heart of neural network training.	
9. Construction of Deep Neural Networks	Defines architectures used in robotics perception and control.	
10. Elimination and LU = A = LU	Foundational for solving linear systems in control and estimation (e.g. Kalman filtering).	

## Tier 2 — High-Value Applied Foundations<br />
### Rank	Topic	Why It’s Important<br />
11. Singular Values & SVD	Used in PCA, control design, dimensionality reduction, and sensor calibration.<br />
12. Compressed Sensing and Matrix Completion	For efficient sensing, sparse representations, and vision reconstruction.	<br />
13. Fourier Transforms: Discrete and Continuous	Used in computer vision, SLAM, and signal processing.	<br />
14. The Four Fundamental Subspaces	Basis for understanding projections, null spaces, control reachability/observability.	<br />
15. Linear Programming, Game Theory, Duality	Core of robotics path planning, resource allocation, and control optimization.	<br />
16. Minimum Problems: Convexity & Newton’s Method	Underpins control optimization, LQR, and MPC methods.	<br />
17. Numerical Linear Algebra	Needed for real-world implementations — stable computations on embedded hardware.	<br />
18. Mean, Variance, and Probability	Basic statistical grounding for all probabilistic algorithms.	<br />
19. Norms of Vectors and Functions	Basis for cost functions, losses, and stability metrics.	<br />
20. Principal Components and Best Low-Rank Matrix	PCA, data compression, control simplifications.	<br />

## Tier 3 — Advanced / Emerging Use Cases (specialized but impactful)
### Rank	Topic	Why It Matters<br />
21. Split Algorithms for f(x) + g(x)	Used in distributed optimization (multi-robot and federated learning).	<br />
22. The Kronecker Product A⊗B	Key in representing complex robotic systems (tensor operations, covariance structures).	<br />
23. Toeplitz Matrices and Shift Invariant Filters	Image processing, convolution operations.	<br />
24. Clustering by Spectral Methods and k-means	Unsupervised learning for perception and environment mapping.	<br />
25. The World of Machine Learning	Conceptual overview tying all methods together — for integration.	
26. Lagrange Multipliers = Derivatives of the Cost	Core to constrained optimization in control and motion planning.	<br />
27. Rapidly Decaying Singular Values	For model compression, low-rank learning, efficient inference.	<br />
28. Completing Rank One Matrices	Useful in structure-from-motion and sensor calibration.	<br />
29. Shift Matrices and Circulant Matrices	Robotics signal processing, convolution kernels.	<br />
30. Symmetric Positive Definite Matrices	Covariance, energy functions, and Lyapunov stability.	<br />

## Tier 4 — Mathematical Infrastructure (supporting topics)
### Rank	Topic	Why It’s Helpful<br />
31. Orthogonal Matrices and Subspaces	Stable transformations, rotations in robotics, SVD foundations.	<br />
32. Multiplication AᵗA and Column Spaces	Needed for least squares, regressions.	<br />
33. Randomized Linear Algebra	Fast matrix ops in big data and large ML models.	<br />
34. Graphs and Laplacians	Used in graph neural networks, swarm robotics, and SLAM.	<br />
35. Orthogonal Procrustes Problem	Pose alignment, 3D reconstruction, robotics calibration.	<br />
36. Moments, Cumulants, and Inequalities	Statistical inference, uncertainty propagation.	<br />
37. Distance Matrices	Used in clustering, SLAM, and manifold learning.	<br />
38. VII.4 Hyperparameters: The Essential Decisions	For practical tuning and optimization.	<br />
39. I.2 Matrix-Matrix Multiplication AB	Core computational operation — assumed known.	
40. I.12 Factoring Matrices and Tensors	Deep latent model interpretability, multi-modal sensor fusion.
