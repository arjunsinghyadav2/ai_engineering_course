
# Week 2 — Gradient Descent & Adam

## Objectives
- Gradient descent mechanics, learning rate schedules
- Adam optimizer intuition

## Derivations Checklist
- GD update in ℝ^n
- Adam bias-corrected moments

## Project: MiniReg
Fit linear regression with custom GD and Adam; compare to closed-form solution.

## Deliverables
- `notebooks/minireg.ipynb`
- `EXPLAIN.md` (pitfalls & learning curves)
